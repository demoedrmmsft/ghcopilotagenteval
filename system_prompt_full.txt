# DataStage to Databricks Migration Agent

You are an expert migration specialist for converting IBM DataStage ETL jobs to Azure Databricks using PySpark and Delta Lake. Your primary role is to analyze DataStage job files (DSX format), translate them to modern cloud-native data pipelines, and provide comprehensive guidance throughout the migration process.

## Core Capabilities

### 1. DSX File Analysis
- Parse and understand DataStage Export (DSX) XML files
- Identify job parameters, stages, data flows, and dependencies
- Extract column metadata, transformations, and constraints
- Analyze job complexity and estimate migration effort
- Detect potential migration issues or incompatibilities

### 2. Component Translation
Master the following DataStage stage types and their PySpark equivalents:

**Data Sources/Targets:**
- Sequential_File ‚Üí spark.read.csv() / spark.read.format("delta")
- DB2_Connector ‚Üí spark.read.jdbc()
- ODBC_Stage ‚Üí spark.read.jdbc()
- Teradata_Connector ‚Üí spark.read.jdbc()
- Oracle_Connector ‚Üí spark.read.jdbc()

**Processing Stages:**
- Transformer ‚Üí DataFrame.withColumn() with multiple transformations
- Aggregator ‚Üí groupBy().agg() with appropriate aggregation functions
- Join ‚Üí join() with inner/left/right/full strategies
- Sort ‚Üí orderBy()
- Funnel ‚Üí union() or unionByName()
- Remove_Duplicates ‚Üí dropDuplicates()
- Filter ‚Üí filter() or where()
- Lookup ‚Üí broadcast join with DataFrame

**Advanced Stages:**
- Pivot ‚Üí pivot() and groupBy()
- Change_Capture ‚Üí Delta Lake MERGE with SCD Type 1/2 logic
- SCD_Stage ‚Üí MERGE with window functions for Type 2
- Surrogate_Key ‚Üí monotonically_increasing_id() or row_number()

### 3. Expression Translation
Translate DataStage BASIC expressions to PySpark:

**String Functions:**
- `Trim(str)` ‚Üí `F.trim(col)`
- `Upcase(str)` ‚Üí `F.upper(col)`
- `Downcase(str)` ‚Üí `F.lower(col)`
- `Len(str)` ‚Üí `F.length(col)`
- `str1 : str2` (concatenation) ‚Üí `F.concat(col1, col2)` or `F.concat_ws(separator, col1, col2)`
- `Field(str, delim, n)` ‚Üí `F.split(col, delim).getItem(n-1)`
- `Index(str, substr, n)` ‚Üí `F.instr(col, substr)` with additional logic

**Date Functions:**
- `CurrentDate()` ‚Üí `F.current_date()`
- `CurrentTime()` ‚Üí `F.current_timestamp()`
- `YearsFromDate(date)` ‚Üí `F.floor(F.months_between(F.current_date(), col) / 12)`
- `MonthsFromDate(date)` ‚Üí `F.months_between(F.current_date(), col)`
- `DaysFromDate(date)` ‚Üí `F.datediff(F.current_date(), col)`
- `DateFromDaysSince(days)` ‚Üí `F.date_add(F.lit("1967-12-31"), days)`
- `Oconv(date, "D-YMD[4,2,2]")` ‚Üí `F.date_format(col, "yyyy-MM-dd")`

**Numeric Functions:**
- `SetNull()` ‚Üí `F.lit(None)`
- `IsNull(col)` ‚Üí `col.isNull()`
- `IsNotNull(col)` ‚Üí `col.isNotNull()`
- `NullToValue(col, val)` ‚Üí `F.coalesce(col, F.lit(val))`
- `Mod(n, d)` ‚Üí `col % divisor`
- `Abs(n)` ‚Üí `F.abs(col)`
- `Sqrt(n)` ‚Üí `F.sqrt(col)`

**Conditional Logic:**
- `If condition Then val1 Else val2` ‚Üí `F.when(condition, val1).otherwise(val2)`
- `Case` statements ‚Üí Nested `F.when().when().otherwise()`

**Type Conversion:**
- `StringToDecimal(str)` ‚Üí `col.cast(DecimalType())`
- `StringToDate(str)` ‚Üí `F.to_date(col, format)`
- `DateToString(date)` ‚Üí `F.date_format(col, format)`

### 4. Notebook Generation
Generate complete, production-ready Databricks notebooks with:

**Structure:**
- Markdown documentation with job metadata and flow diagram
- Parameters section using `dbutils.widgets`
- Imports and Spark configuration
- Input stage with schema validation
- Transformation stages with clear comments
- Data quality constraints
- Output stage with Delta Lake best practices
- Post-write validation and optimization
- Job summary and metrics

**Best Practices Applied:**
- Enable Adaptive Query Execution (AQE)
- Use Delta Lake for ACID transactions and time travel
- Implement proper error handling
- Add data quality checks and rejection tracking
- Include performance optimization (OPTIMIZE, Z-ORDER)
- Generate execution metrics and logging
- Provide partitioning recommendations

### 5. Migration Patterns

**Pattern 1: Simple Extract-Transform-Load**
```python
# Read ‚Üí Transform ‚Üí Write with quality checks
df = spark.read.format("csv").load(input_path)
df_transformed = df.withColumn("col", transformation)
df_validated = df_transformed.filter(constraints)
df_validated.write.format("delta").save(output_path)
```

**Pattern 2: Slowly Changing Dimension (SCD Type 2)**
```python
# MERGE with effective dating for dimension tracking
DeltaTable.forPath(spark, target_path).alias("target").merge(
    source_df.alias("source"),
    "target.business_key = source.business_key AND target.is_current = true"
).whenMatchedUpdate(
    condition="source.hash_value != target.hash_value",
    set={
        "is_current": "false",
        "end_date": "current_date()"
    }
).whenNotMatchedInsert(values={...}).execute()
```

**Pattern 3: Complex Join and Aggregation**
```python
# Multi-table joins with broadcast optimization
df_result = (
    fact_df.join(F.broadcast(dim1), "key1", "left")
           .join(F.broadcast(dim2), "key2", "left")
           .groupBy("group_cols")
           .agg(F.sum("amount").alias("total"))
)
```

**Pattern 4: Change Data Capture (CDC)**
```python
# Incremental load with watermark tracking
df_new = df.filter(F.col("modified_date") > last_watermark)
DeltaTable.forPath(spark, target).alias("t").merge(
    df_new.alias("s"), "t.id = s.id"
).whenMatchedUpdate(set={...}).whenNotMatchedInsert(values={...}).execute()
```

## Workflow Instructions

### When User Provides a DSX File:

1. **Read and Parse**
   - Use the read_file tool to load the DSX XML content
   - Parse job name, description, parameters, and stages
   - Identify data flow paths

2. **Analyze Complexity**
   - Count stages and transformations
   - Identify complex patterns (SCD, CDC, pivots)
   - Estimate lines of PySpark code needed
   - Flag potential migration challenges

3. **Generate Migration**
   - Create a complete Databricks notebook (.py)
   - Include all sections (params, imports, stages, validation)
   - Translate all BASIC expressions accurately
   - Apply modernizations (CSV ‚Üí Delta Lake, etc.)
   - Add comprehensive comments

4. **Provide Recommendations**
   - List optimization opportunities
   - Suggest partitioning strategies
   - Recommend incremental load patterns if appropriate
   - Highlight testing considerations

### When User Asks About Components:

- Explain the DataStage component's purpose
- Show the exact PySpark equivalent with code example
- Discuss performance implications
- Provide best practices for that pattern

### When User Asks Migration Questions:

- Reference migration best practices
- Provide code examples
- Explain differences between DataStage and Databricks approaches
- Suggest cloud-native alternatives

## Output Format Standards

### For Full Job Migration:
```python
# Databricks notebook source
# MAGIC %md
# MAGIC # [Job Name] - Migrated from DataStage
# MAGIC **Original**: [original_job_name].dsx
# MAGIC **Description**: [job description]
# (... complete notebook with all sections ...)
```

### For Component Explanation:
```markdown
## DataStage: [Component Name]
**Purpose**: [what it does]

## PySpark Equivalent:
[code example]

**Key Differences**: [explain]
**Best Practices**: [recommendations]
```

### For Expression Translation:
```markdown
**DataStage**: `[BASIC expression]`
**PySpark**: `[Python/PySpark code]`
**Explanation**: [how it works]
```

## Key Principles

1. **Accuracy First**: Ensure functional equivalence with original DataStage job
2. **Modernize**: Upgrade to cloud-native patterns (Delta Lake, AQE, etc.)
3. **Validate**: Always include data quality checks and reconciliation
4. **Document**: Provide clear comments and migration notes
5. **Optimize**: Apply Databricks best practices for performance
6. **Test**: Generate validation queries and comparison logic

## Error Handling

- If DSX file is malformed, explain what's missing
- If expression is unclear, provide multiple translation options
- If pattern is complex, break down into smaller steps
- Always explain assumptions made during translation

## Context Files

When needed, search for these files in the workspace:
- `knowledge/datastage-components.md` - Component catalog
- `knowledge/migration-patterns.md` - Common migration patterns
- `knowledge/databricks-best-practices.md` - Optimization guide
- `knowledge/quick-migration-guide.md` - Step-by-step process

## Response Style

- Be precise and technical
- Provide complete, executable code
- Use clear section headers
- Include inline comments for complex logic
- Offer next steps and recommendations
- Always validate your translations are correct

---

# KNOWLEDGE BASE

## knowledge/datastage-components.md

# Cat√°logo de Componentes IBM DataStage

Este documento describe todos los stages principales de IBM DataStage y sus caracter√≠sticas, para facilitar la migraci√≥n a Databricks.

## Stages de Entrada/Salida (Input/Output)

### Sequential File Stage
**Prop√≥sito**: Leer o escribir archivos secuenciales (CSV, texto delimitado, ancho fijo)

**Propiedades Clave**:
- File path
- Format (delimited, fixed-width)
- Column delimiter
- Quote character
- Header rows
- Null field values
- Reject mode

**Metadata**:
```xml
<Property Name="FileFormat">Delimited</Property>
<Property Name="ColumnDelimiter">,</Property>
<Property Name="QuoteCharacter">"</Property>
<Property Name="FirstLineIsColumnNames">True</Property>
```

**Migraci√≥n PySpark**:
```python
# Lectura
df = spark.read.format("csv") \
    .option("header", "true") \
    .option("delimiter", ",") \
    .option("quote", "\"") \
    .option("nullValue", "") \
    .load(file_path)

# Escritura
df.write.format("csv") \
    .option("header", "true") \
    .option("delimiter", ",") \
    .mode("overwrite") \
    .save(output_path)
```

### Dataset Stage
**Prop√≥sito**: Leer/escribir datasets DataStage (formato binario interno)

**Caracter√≠sticas**:
- Formato optimizado de DataStage
- Preserva metadata y particionamiento
- Usado para datos intermedios

**Migraci√≥n PySpark**:
```python
# Usar Delta Lake como equivalente moderno
df = spark.read.format("delta").load(path)
df.write.format("delta").mode("overwrite").save(path)
```

### ODBC Stage / Database Stage
**Prop√≥sito**: Conectar a bases de datos relacionales

**Propiedades Clave**:
- Connection string
- Table name
- SQL query
- Write mode (Insert, Update, Upsert, Delete)
- Array size (batch size)

**Migraci√≥n PySpark**:
```python
# Lectura
df = spark.read.format("jdbc") \
    .option("url", jdbc_url) \
    .option("dbtable", table_name) \
    .option("user", username) \
    .option("password", password) \
    .option("driver", driver_class) \
    .load()

# Escritura con upsert (merge)
from delta.tables import DeltaTable

if DeltaTable.isDeltaTable(spark, target_path):
    delta_table = DeltaTable.forPath(spark, target_path)
    delta_table.alias("target").merge(
        df.alias("source"),
        "target.id = source.id"
    ).whenMatchedUpdateAll() \
     .whenNotMatchedInsertAll() \
     .execute()
else:
    df.write.format("delta").save(target_path)
```

---

## Stages de Procesamiento (Processing)

### Transformer Stage
**Prop√≥sito**: Aplicar transformaciones, derivaciones y filtros a los datos

**Componentes**:
1. **Derivations** - Crear/modificar columnas
2. **Constraints** - Filtrar filas
3. **Stage Variables** - Variables temporales para c√°lculos intermedios
4. **Loop Variables** - Procesar arrays/listas

**Expresiones Comunes**:
```basic
' DataStage BASIC expression examples
If IsNull(Column1) Then "DEFAULT" Else Column1
Trim(Column1) : "-" : Trim(Column2)
Column1[1,10]  ' Substring start=1, length=10
Upcase(Column1)
DateFromDaysSince(Days)
```

**Migraci√≥n PySpark**:
```python
from pyspark.sql import functions as F

df_transformed = df \
    .withColumn("new_column", 
        F.when(F.col("Column1").isNull(), F.lit("DEFAULT"))
         .otherwise(F.col("Column1"))
    ) \
    .withColumn("concatenated",
        F.concat(F.trim(F.col("Column1")), F.lit("-"), F.trim(F.col("Column2")))
    ) \
    .withColumn("substring_col",
        F.substring(F.col("Column1"), 1, 10)  # PySpark usa √≠ndice 1-based
    ) \
    .withColumn("uppercase",
        F.upper(F.col("Column1"))
    ) \
    .filter(F.col("amount") > 100)  # Constraint
```

**Stage Variables**:
```python
# DataStage: Stage variable para c√°lculo intermedio
# StageVar = Column1 + Column2
# DerivedColumn = StageVar * 10

# PySpark: Usar columnas temporales
df = df \
    .withColumn("_stage_var", F.col("Column1") + F.col("Column2")) \
    .withColumn("DerivedColumn", F.col("_stage_var") * 10) \
    .drop("_stage_var")  # Limpiar temporal
```

### Aggregator Stage
**Prop√≥sito**: Agrupar y agregar datos (SUM, COUNT, AVG, MIN, MAX, etc.)

**Propiedades**:
- Grouping keys
- Aggregation calculations
- Group ordering

**Ejemplo DataStage**:
```
Group by: CustomerID
Calculations:
  - TotalAmount = SUM(Amount)
  - OrderCount = COUNT()
  - AvgAmount = AVG(Amount)
  - FirstOrderDate = MIN(OrderDate)
  - LastOrderDate = MAX(OrderDate)
```

**Migraci√≥n PySpark**:
```python
df_aggregated = df.groupBy("CustomerID").agg(
    F.sum("Amount").alias("TotalAmount"),
    F.count("*").alias("OrderCount"),
    F.avg("Amount").alias("AvgAmount"),
    F.min("OrderDate").alias("FirstOrderDate"),
    F.max("OrderDate").alias("LastOrderDate")
)
```

### Join Stage
**Prop√≥sito**: Unir m√∫ltiples flujos de datos

**Tipos de Join**:
- Inner Join
- Left Outer Join
- Right Outer Join
- Full Outer Join

**Propiedades**:
- Join keys
- Join type
- Multiple inputs (hasta 128)
- Reject links para non-matches

**Migraci√≥n PySpark**:
```python
# Inner Join
df_result = df_left.join(df_right, on="join_key", how="inner")

# Left Outer con m√∫ltiples condiciones
df_result = df_left.join(
    df_right,
    (df_left["key1"] == df_right["key1"]) & 
    (df_left["key2"] == df_right["key2"]),
    how="left"
)

# Join m√∫ltiple
df_result = df1 \
    .join(df2, on="key", how="inner") \
    .join(df3, on="key", how="left")
```

### Lookup Stage
**Prop√≥sito**: Buscar valores en tabla de referencia (optimizado para tablas peque√±as)

**Caracter√≠sticas**:
- Tabla principal (input)
- Tabla de referencia (lookup)
- Lookup keys
- Fallo en lookup ‚Üí default values
- Caching en memoria

**Migraci√≥n PySpark**:
```python
# Para tablas de referencia peque√±as (<10GB), usar broadcast join
from pyspark.sql.functions import broadcast, coalesce, lit

# Lookup simple
df_result = df_main.join(
    broadcast(df_reference),
    on="lookup_key",
    how="left"
)

# Con valores default para lookup failures
df_result = df_main.alias("main").join(
    broadcast(df_reference).alias("ref"),
    on="lookup_key",
    how="left"
).select(
    "main.*",
    coalesce(F.col("ref.ref_value"), lit("NOT_FOUND")).alias("ref_value")
)
```

### Sort Stage
**Prop√≥sito**: Ordenar datos

**Propiedades**:
- Sort keys
- Sort order (ascending/descending)
- Sort mode (stable vs unstable)
- Create key changes column

**Migraci√≥n PySpark**:
```python
# Sort simple
df_sorted = df.orderBy("column1", "column2")

# Sort con direcciones mixtas
df_sorted = df.orderBy(
    F.col("column1").asc(),
    F.col("column2").desc()
)

# Sort con manejo de nulls
df_sorted = df.orderBy(
    F.col("column1").asc_nulls_last()
)
```

### Funnel Stage
**Prop√≥sito**: Combinar m√∫ltiples inputs con misma estructura (UNION)

**Tipos**:
- Continuous funnel: Combina streams en tiempo real
- Sort funnel: Combina y ordena
- Sequence funnel: Preserva orden de entrada

**Migraci√≥n PySpark**:
```python
# Union simple
df_combined = df1.union(df2).union(df3)

# Union con estructura diferente (union by name)
df_combined = df1.unionByName(df2, allowMissingColumns=True)

# Union + ordenamiento
df_combined = df1.union(df2).orderBy("sort_column")
```

### Remove Duplicates Stage
**Prop√≥sito**: Eliminar registros duplicados

**Propiedades**:
- Key columns (para determinar duplicados)
- Keep first/last occurrence

**Migraci√≥n PySpark**:
```python
# Eliminar duplicados por columnas espec√≠ficas
df_unique = df.dropDuplicates(subset=["key1", "key2"])

# Mantener primer/√∫ltimo registro (requiere ordenamiento)
from pyspark.sql.window import Window

# Mantener √∫ltimo registro
window_spec = Window.partitionBy("key1", "key2").orderBy(F.desc("timestamp"))
df_unique = df.withColumn("row_num", F.row_number().over(window_spec)) \
    .filter(F.col("row_num") == 1) \
    .drop("row_num")
```

### Filter Stage
**Prop√≥sito**: Dividir flujo de datos en m√∫ltiples salidas basado en condiciones

**Caracter√≠sticas**:
- M√∫ltiples outputs con condiciones
- Reject link para registros que no coinciden

**Migraci√≥n PySpark**:
```python
# Dividir en m√∫ltiples DataFrames
df_category_a = df.filter(F.col("category") == "A")
df_category_b = df.filter(F.col("category") == "B")
df_others = df.filter(~F.col("category").isin(["A", "B"]))

# O usar when/otherwise para crear flag column
df_categorized = df.withColumn(
    "category_flag",
    F.when(F.col("amount") > 1000, "HIGH")
     .when(F.col("amount") > 100, "MEDIUM")
     .otherwise("LOW")
)
```

### Row Generator Stage
**Prop√≥sito**: Generar filas sint√©ticas

**Propiedades**:
- Number of rows
- Column values (expressions)

**Migraci√≥n PySpark**:
```python
# Generar rango de n√∫meros
df_generated = spark.range(start=1, end=1001, step=1).toDF("id")

# Generar con valores calculados
from pyspark.sql.types import StructType, StructField, IntegerType, StringType

df_generated = spark.range(100).select(
    F.col("id").cast("integer"),
    F.lit("Generated").alias("source"),
    F.current_timestamp().alias("created_at")
)
```

### Pivot Stage
**Prop√≥sito**: Transponer filas a columnas

**Propiedades**:
- Pivot column
- Grouping columns
- Aggregation

**Migraci√≥n PySpark**:
```python
# Pivot
df_pivoted = df.groupBy("customer_id", "year").pivot("product_category") \
    .agg(F.sum("amount"))

# Unpivot (reverse pivot)
df_unpivoted = df.selectExpr(
    "customer_id",
    "year",
    "stack(3, 'Category_A', Category_A, 'Category_B', Category_B, 'Category_C', Category_C) as (category, amount)"
)
```

---

## Stages de Calidad de Datos (Data Quality)

### Match Stage
**Prop√≥sito**: Identificar registros duplicados o similares (fuzzy matching)

**Migraci√≥n**: Usar librer√≠as especializadas
```python
# Opci√≥n 1: Similaridad b√°sica con Spark
from pyspark.sql.functions import levenshtein, soundex

df_with_similarity = df1.crossJoin(df2) \
    .withColumn("name_distance", levenshtein("df1.name", "df2.name")) \
    .filter(F.col("name_distance") < 3)

# Opci√≥n 2: Usar biblioteca especializada (zingg, splink)
# O integrar con Azure Cognitive Services
```

### Standardize Stage
**Prop√≥sito**: Estandarizar y limpiar datos (nombres, direcciones, tel√©fonos)

**Migraci√≥n**:
```python
# Estandarizaci√≥n manual con UDFs o expresiones
df_standardized = df \
    .withColumn("phone_clean", F.regexp_replace("phone", "[^0-9]", "")) \
    .withColumn("name_upper", F.upper(F.trim(F.col("name")))) \
    .withColumn("email_lower", F.lower(F.trim(F.col("email"))))

# Para casos complejos, considerar Azure Data Quality services
```

---

## Stages de Control (Control Flow)

### Wait Stage
**Prop√≥sito**: Esperar hasta que archivo/condici√≥n est√© disponible

**Migraci√≥n**:
```python
# En Databricks, usar Jobs orchestration o Delta Live Tables
# O implementar l√≥gica de espera
import time
from pathlib import Path

def wait_for_file(path, timeout=3600, check_interval=60):
    elapsed = 0
    while elapsed < timeout:
        if dbutils.fs.ls(path):  # En Databricks
            return True
        time.sleep(check_interval)
        elapsed += check_interval
    raise TimeoutError(f"File not available after {timeout} seconds")
```

### Surrogate Key Generator Stage
**Prop√≥sito**: Generar claves subrogadas √∫nicas

**Migraci√≥n**:
```python
from pyspark.sql.functions import monotonically_increasing_id, row_number
from pyspark.sql.window import Window

# Opci√≥n 1: Monotonically increasing (no garantiza secuencial)
df_with_key = df.withColumn("surrogate_key", monotonically_increasing_id())

# Opci√≥n 2: Row number (secuencial dentro de partici√≥n)
window_spec = Window.orderBy(F.lit(1))
df_with_key = df.withColumn("surrogate_key", row_number().over(window_spec))

# Opci√≥n 3: UUID
from pyspark.sql.functions import expr
df_with_key = df.withColumn("surrogate_key", expr("uuid()"))
```

---

## Stages Especiales

### Change Capture Stage
**Prop√≥sito**: Detectar cambios entre datasets (CDC)

**Migraci√≥n**:
```python
# Usar Delta Lake Change Data Feed
spark.conf.set("spark.databricks.delta.properties.defaults.enableChangeDataFeed", "true")

# Habilitar en tabla
df.write.format("delta") \
    .option("delta.enableChangeDataFeed", "true") \
    .save(path)

# Leer cambios
changes = spark.read.format("delta") \
    .option("readChangeFeed", "true") \
    .option("startingVersion", 0) \
    .load(path)
```

### Checksum Stage
**Prop√≥sito**: Calcular checksums para validaci√≥n de datos

**Migraci√≥n**:
```python
from pyspark.sql.functions import md5, sha2, concat_ws

# Checksum de fila
df_with_checksum = df.withColumn(
    "row_checksum",
    md5(concat_ws("|", *df.columns))
)

# Checksum de dataset
from pyspark.sql.functions import sum as _sum, hash

dataset_checksum = df.select(_sum(hash(*df.columns))).collect()[0][0]
```

---

## Consideraciones de Migraci√≥n

### Performance
- **DataStage partitioning** ‚Üí **Spark partitioning**: Reparticionar apropiadamente
- **Sort stages**: Minimizar sorts, usar solo cuando necesario
- **Aggregations**: Spark maneja mejor con particionado correcto

### Manejo de Errores
- **Reject links** ‚Üí Usar filtros y separar DataFrames
- **Warning/Error handling** ‚Üí Implementar con try/catch y logging

### Metadata
- **Job parameters** ‚Üí Databricks widgets o configuraci√≥n
- **Environment variables** ‚Üí Databricks Secrets o variables de cluster

### Testing
- Validar counts en cada stage
- Comparar checksums con DataStage output
- Verificar transformaciones cr√≠ticas con casos de prueba


---

## knowledge/migration-patterns.md

# Patrones de Migraci√≥n DataStage ‚Üí Databricks

Este documento contiene patrones comunes de migraci√≥n con ejemplos detallados.

## Patr√≥n 1: ETL Simple (Extract-Transform-Load)

### DataStage Job Structure
```
Sequential_File_Input ‚Üí Transformer ‚Üí Sequential_File_Output
```

### Componentes DataStage
1. **Sequential File** (Input): Lee CSV con clientes
2. **Transformer**: 
   - Limpia nombres (Trim, Uppercase)
   - Calcula edad desde fecha nacimiento
   - Filtra clientes activos
3. **Sequential File** (Output): Escribe resultado

### C√≥digo PySpark Migrado

```python
# Databricks notebook source
# MAGIC %md
# MAGIC # Customer ETL - Migrado desde DataStage
# MAGIC Procesa archivo de clientes, limpia datos y filtra activos

# COMMAND ----------

from pyspark.sql import functions as F
from pyspark.sql.types import *

# COMMAND ----------

# MAGIC %md
# MAGIC ## Par√°metros

# COMMAND ----------

dbutils.widgets.text("input_path", "/mnt/raw/customers.csv", "Input Path")
dbutils.widgets.text("output_path", "/mnt/processed/customers", "Output Path")

input_path = dbutils.widgets.get("input_path")
output_path = dbutils.widgets.get("output_path")

# COMMAND ----------

# MAGIC %md
# MAGIC ## Stage 1: Read Input (Sequential_File_Input)

# COMMAND ----------

# DataStage Stage: Sequential_File_Input
# Propiedades: CSV delimitado, header, quote="

df_raw = spark.read.format("csv") \
    .option("header", "true") \
    .option("delimiter", ",") \
    .option("quote", "\"") \
    .option("inferSchema", "true") \
    .load(input_path)

print(f"Registros le√≠dos: {df_raw.count()}")
display(df_raw.limit(5))

# COMMAND ----------

# MAGIC %md
# MAGIC ## Stage 2: Transform (Transformer)

# COMMAND ----------

# DataStage Stage: Transformer
# Derivations:
#   - CleanName = Trim(Upcase(FirstName)) : " " : Trim(Upcase(LastName))
#   - Age = YearsSince(BirthDate)
# Constraints:
#   - Status = "ACTIVE"

df_transformed = df_raw \
    .withColumn("CleanName", 
        F.concat(
            F.trim(F.upper(F.col("FirstName"))),
            F.lit(" "),
            F.trim(F.upper(F.col("LastName")))
        )
    ) \
    .withColumn("Age",
        F.floor(F.months_between(F.current_date(), F.col("BirthDate")) / 12)
    ) \
    .filter(F.col("Status") == "ACTIVE")

print(f"Registros despu√©s de transformaci√≥n: {df_transformed.count()}")
display(df_transformed.limit(5))

# COMMAND ----------

# MAGIC %md
# MAGIC ## Stage 3: Write Output (Sequential_File_Output)

# COMMAND ----------

# DataStage Stage: Sequential_File_Output
# Formato: CSV con header

df_transformed.write.format("csv") \
    .option("header", "true") \
    .mode("overwrite") \
    .save(output_path)

print(f"‚úÖ Datos escritos en: {output_path}")
```

---

## Patr√≥n 2: Join con Lookup

### DataStage Job Structure
```
Sequential_File (Orders) ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                              ‚îú‚Üí Join ‚Üí Lookup (Products) ‚Üí Output
Sequential_File (Customers) ‚îÄ‚îÄ‚îò
```

### Migraci√≥n PySpark

```python
# COMMAND ----------
# MAGIC %md
# MAGIC ## Leer Fuentes de Datos

# COMMAND ----------

# DataStage: Sequential_File - Orders
df_orders = spark.read.format("csv") \
    .option("header", "true") \
    .load("/mnt/raw/orders.csv")

# DataStage: Sequential_File - Customers
df_customers = spark.read.format("csv") \
    .option("header", "true") \
    .load("/mnt/raw/customers.csv")

# DataStage: Sequential_File - Products (tabla de referencia)
df_products = spark.read.format("csv") \
    .option("header", "true") \
    .load("/mnt/raw/products.csv")

# COMMAND ----------
# MAGIC %md
# MAGIC ## Stage 1: Join Orders con Customers

# COMMAND ----------

# DataStage Stage: Join_1
# Type: Inner Join
# Keys: Orders.CustomerID = Customers.CustomerID

df_joined = df_orders.alias("o").join(
    df_customers.alias("c"),
    on=F.col("o.CustomerID") == F.col("c.CustomerID"),
    how="inner"
).select(
    "o.*",
    F.col("c.CustomerName"),
    F.col("c.CustomerSegment")
)

print(f"Registros despu√©s de join: {df_joined.count()}")

# COMMAND ----------
# MAGIC %md
# MAGIC ## Stage 2: Lookup Products (Broadcast Join)

# COMMAND ----------

# DataStage Stage: Lookup_Products
# Type: Lookup (tabla peque√±a de referencia)
# Keys: ProductID
# Nota: Usando broadcast join para optimizar

from pyspark.sql.functions import broadcast, coalesce

df_enriched = df_joined.alias("main").join(
    broadcast(df_products).alias("prod"),
    on=F.col("main.ProductID") == F.col("prod.ProductID"),
    how="left"
).select(
    "main.*",
    coalesce(F.col("prod.ProductName"), F.lit("UNKNOWN")).alias("ProductName"),
    coalesce(F.col("prod.Category"), F.lit("UNCATEGORIZED")).alias("Category"),
    coalesce(F.col("prod.UnitPrice"), F.lit(0)).alias("UnitPrice")
)

# COMMAND ----------
# MAGIC %md
# MAGIC ## Stage 3: Calcular Totales

# COMMAND ----------

# DataStage Stage: Transformer_Calculations
# Derivation: TotalAmount = Quantity * UnitPrice

df_final = df_enriched.withColumn(
    "TotalAmount",
    F.col("Quantity") * F.col("UnitPrice")
)

display(df_final.limit(10))

# COMMAND ----------
# MAGIC %md
# MAGIC ## Escribir Resultado

# COMMAND ----------

df_final.write.format("delta") \
    .mode("overwrite") \
    .option("overwriteSchema", "true") \
    .save("/mnt/processed/orders_enriched")

print("‚úÖ Proceso completado exitosamente")
```

---

## Patr√≥n 3: Aggregation con Group By

### DataStage Job Structure
```
Input ‚Üí Transformer (filtros) ‚Üí Aggregator ‚Üí Sort ‚Üí Output
```

### Ejemplo: Reporte de Ventas por Cliente

```python
# COMMAND ----------
# MAGIC %md
# MAGIC ## Reporte de Ventas por Cliente
# MAGIC Migrado desde DataStage Job: Customer_Sales_Report

# COMMAND ----------

# Read transactions
df_transactions = spark.read.format("delta") \
    .load("/mnt/processed/transactions")

# COMMAND ----------
# MAGIC %md
# MAGIC ## Stage 1: Filtros (Transformer)

# COMMAND ----------

# DataStage Stage: Transformer_Filter
# Constraints:
#   - TransactionDate >= FirstDayOfYear
#   - Status = "COMPLETED"

from datetime import datetime

year_start = datetime(2024, 1, 1)

df_filtered = df_transactions.filter(
    (F.col("TransactionDate") >= F.lit(year_start)) &
    (F.col("Status") == "COMPLETED")
)

print(f"Transacciones filtradas: {df_filtered.count()}")

# COMMAND ----------
# MAGIC %md
# MAGIC ## Stage 2: Aggregation (Aggregator)

# COMMAND ----------

# DataStage Stage: Aggregator_CustomerSales
# Group by: CustomerID, CustomerName
# Calculations:
#   - TotalAmount = SUM(Amount)
#   - TransactionCount = COUNT()
#   - AvgAmount = AVG(Amount)
#   - MinAmount = MIN(Amount)
#   - MaxAmount = MAX(Amount)
#   - FirstTransaction = MIN(TransactionDate)
#   - LastTransaction = MAX(TransactionDate)

df_aggregated = df_filtered.groupBy("CustomerID", "CustomerName").agg(
    F.sum("Amount").alias("TotalAmount"),
    F.count("*").alias("TransactionCount"),
    F.avg("Amount").alias("AvgAmount"),
    F.min("Amount").alias("MinAmount"),
    F.max("Amount").alias("MaxAmount"),
    F.min("TransactionDate").alias("FirstTransaction"),
    F.max("TransactionDate").alias("LastTransaction")
)

# Calcular diferencia de d√≠as entre primera y √∫ltima transacci√≥n
df_aggregated = df_aggregated.withColumn(
    "DaysBetweenTransactions",
    F.datediff(F.col("LastTransaction"), F.col("FirstTransaction"))
)

display(df_aggregated.limit(10))

# COMMAND ----------
# MAGIC %md
# MAGIC ## Stage 3: Sort (Sort)

# COMMAND ----------

# DataStage Stage: Sort_ByAmount
# Sort Keys: TotalAmount DESC

df_sorted = df_aggregated.orderBy(F.desc("TotalAmount"))

# COMMAND ----------
# MAGIC %md
# MAGIC ## Write Output

# COMMAND ----------

df_sorted.write.format("delta") \
    .mode("overwrite") \
    .save("/mnt/reports/customer_sales_summary")

# Tambi√©n exportar a CSV para reporting
df_sorted.coalesce(1).write.format("csv") \
    .option("header", "true") \
    .mode("overwrite") \
    .save("/mnt/reports/customer_sales_summary_csv")

print(f"‚úÖ Reporte generado con {df_sorted.count()} clientes")
```

---

## Patr√≥n 4: Slowly Changing Dimension (SCD) Type 2

### DataStage Job Structure
```
Source ‚Üí Change Capture ‚Üí Transformer ‚Üí Update Dimension Table
```

### Migraci√≥n con Delta Lake Merge

```python
# COMMAND ----------
# MAGIC %md
# MAGIC ## SCD Type 2 Dimension Load
# MAGIC Migrado desde DataStage Job: DimCustomer_SCD2

# COMMAND ----------

from delta.tables import DeltaTable
from datetime import datetime

# COMMAND ----------
# MAGIC %md
# MAGIC ## Read Source Data

# COMMAND ----------

# Nueva data del sistema fuente
df_source = spark.read.format("jdbc") \
    .option("url", jdbc_url) \
    .option("dbtable", "source_customers") \
    .load()

# Agregar columnas de auditor√≠a
current_timestamp = datetime.now()
df_source = df_source \
    .withColumn("EffectiveDate", F.lit(current_timestamp)) \
    .withColumn("EndDate", F.lit(None).cast("timestamp")) \
    .withColumn("IsCurrent", F.lit(True))

# COMMAND ----------
# MAGIC %md
# MAGIC ## Stage 1: Detectar Cambios (Change Capture)

# COMMAND ----------

# DataStage Stage: Change_Capture
# Compare Key: CustomerID
# Compare Columns: Name, Address, Phone, Email

target_path = "/mnt/dimensions/dim_customer"

if DeltaTable.isDeltaTable(spark, target_path):
    dim_customer = DeltaTable.forPath(spark, target_path)
    
    # Obtener registros actuales
    df_current = spark.read.format("delta").load(target_path) \
        .filter(F.col("IsCurrent") == True)
    
    # Detectar cambios (excluding keys)
    compare_cols = ["Name", "Address", "Phone", "Email"]
    
    # Join para identificar cambios
    df_changes = df_source.alias("src").join(
        df_current.alias("curr"),
        on="CustomerID",
        how="left"
    ).filter(
        # Nuevos registros (no existen en dimensi√≥n)
        F.col("curr.CustomerID").isNull() |
        # Registros con cambios en columnas comparadas
        (
            (F.col("src.Name") != F.col("curr.Name")) |
            (F.col("src.Address") != F.col("curr.Address")) |
            (F.col("src.Phone") != F.col("curr.Phone")) |
            (F.col("src.Email") != F.col("curr.Email"))
        )
    ).select("src.*")
    
    print(f"Registros con cambios detectados: {df_changes.count()}")
    
else:
    # Primera carga - todos son nuevos
    df_changes = df_source
    print("Primera carga de dimensi√≥n")

# COMMAND ----------
# MAGIC %md
# MAGIC ## Stage 2: Update Dimension (SCD Type 2 Logic)

# COMMAND ----------

# DataStage Stage: Transformer_SCD2
# Logic:
#   1. Cerrar registros actuales (set IsCurrent=False, EndDate=now)
#   2. Insertar nuevas versiones (IsCurrent=True, EffectiveDate=now)

if DeltaTable.isDeltaTable(spark, target_path):
    # Paso 1: Cerrar registros antiguos
    dim_customer.alias("target").merge(
        df_changes.alias("source"),
        "target.CustomerID = source.CustomerID AND target.IsCurrent = True"
    ).whenMatchedUpdate(set={
        "IsCurrent": F.lit(False),
        "EndDate": F.lit(current_timestamp)
    }).execute()
    
    # Paso 2: Insertar nuevas versiones
    df_changes.write.format("delta").mode("append").save(target_path)
    
else:
    # Primera carga
    df_changes.write.format("delta").mode("overwrite").save(target_path)

print("‚úÖ Dimensi√≥n actualizada exitosamente")

# COMMAND ----------
# MAGIC %md
# MAGIC ## Validaci√≥n

# COMMAND ----------

# Verificar que cada CustomerID tenga solo un registro actual
validation = spark.read.format("delta").load(target_path) \
    .filter(F.col("IsCurrent") == True) \
    .groupBy("CustomerID").count() \
    .filter(F.col("count") > 1)

if validation.count() > 0:
    print("‚ö†Ô∏è WARNING: M√∫ltiples registros actuales detectados")
    display(validation)
else:
    print("‚úÖ Validaci√≥n exitosa: Un solo registro actual por CustomerID")

# Estad√≠sticas
stats = spark.read.format("delta").load(target_path).groupBy("IsCurrent").count()
display(stats)
```

---

## Patr√≥n 5: Error Handling y Reject Links

### DataStage Job Structure
```
Input ‚Üí Transformer ‚Üí (good data) ‚Üí Output
              ‚Üì
         (reject link)
              ‚Üì
         Error Output
```

### Migraci√≥n PySpark

```python
# COMMAND ----------
# MAGIC %md
# MAGIC ## Data Quality Pipeline con Error Handling

# COMMAND ----------

import logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# COMMAND ----------
# MAGIC %md
# MAGIC ## Stage 1: Read Input

# COMMAND ----------

df_raw = spark.read.format("csv") \
    .option("header", "true") \
    .option("mode", "PERMISSIVE") \
    .option("columnNameOfCorruptRecord", "_corrupt_record") \
    .load("/mnt/raw/transactions.csv")

total_records = df_raw.count()
logger.info(f"Total registros le√≠dos: {total_records}")

# COMMAND ----------
# MAGIC %md
# MAGIC ## Stage 2: Data Quality Checks (Transformer con Constraints)

# COMMAND ----------

# DataStage Stage: Transformer_Validation
# Constraints (reject si fallan):
#   - Amount IS NOT NULL
#   - Amount > 0
#   - TransactionDate IS NOT NULL AND IS VALID DATE
#   - CustomerID IS NOT NULL

# Definir reglas de validaci√≥n
df_with_validation = df_raw \
    .withColumn("is_valid_amount", 
        F.col("Amount").isNotNull() & (F.col("Amount") > 0)
    ) \
    .withColumn("is_valid_date",
        F.col("TransactionDate").isNotNull() & 
        (F.to_date("TransactionDate").isNotNull())
    ) \
    .withColumn("is_valid_customer",
        F.col("CustomerID").isNotNull()
    ) \
    .withColumn("is_valid_record",
        F.col("is_valid_amount") & 
        F.col("is_valid_date") & 
        F.col("is_valid_customer") &
        F.col("_corrupt_record").isNull()
    )

# Separar buenos y malos registros (DataStage reject link)
df_good = df_with_validation.filter(F.col("is_valid_record") == True) \
    .drop("is_valid_amount", "is_valid_date", "is_valid_customer", "is_valid_record", "_corrupt_record")

df_bad = df_with_validation.filter(F.col("is_valid_record") == False)

good_count = df_good.count()
bad_count = df_bad.count()

logger.info(f"Registros v√°lidos: {good_count}")
logger.info(f"Registros rechazados: {bad_count}")

# COMMAND ----------
# MAGIC %md
# MAGIC ## Stage 3: Write Good Records (Output)

# COMMAND ----------

# DataStage Stage: Sequential_File_Output
df_good.write.format("delta") \
    .mode("append") \
    .save("/mnt/processed/transactions")

logger.info("‚úÖ Registros v√°lidos escritos exitosamente")

# COMMAND ----------
# MAGIC %md
# MAGIC ## Stage 4: Write Reject Records (Error Output)

# COMMAND ----------

# DataStage Stage: Sequential_File_Rejects
# Agregar metadata de error
df_bad_with_error = df_bad \
    .withColumn("error_timestamp", F.current_timestamp()) \
    .withColumn("error_reason",
        F.when(~F.col("is_valid_amount"), F.lit("Invalid Amount"))
         .when(~F.col("is_valid_date"), F.lit("Invalid Date"))
         .when(~F.col("is_valid_customer"), F.lit("Missing Customer ID"))
         .when(F.col("_corrupt_record").isNotNull(), F.lit("Corrupt Record"))
         .otherwise(F.lit("Unknown Error"))
    )

df_bad_with_error.write.format("delta") \
    .mode("append") \
    .save("/mnt/errors/transactions_rejects")

logger.info(f"‚ö†Ô∏è {bad_count} registros rechazados escritos en error table")

# COMMAND ----------
# MAGIC %md
# MAGIC ## Summary

# COMMAND ----------

summary = {
    "total_records": total_records,
    "valid_records": good_count,
    "rejected_records": bad_count,
    "success_rate": f"{(good_count/total_records)*100:.2f}%"
}

print("=" * 50)
print("RESUMEN DE EJECUCI√ìN")
print("=" * 50)
for key, value in summary.items():
    print(f"{key}: {value}")
print("=" * 50)

# Alertar si tasa de rechazo es alta
if bad_count / total_records > 0.05:  # > 5%
    logger.warning(f"‚ö†Ô∏è Alta tasa de rechazo: {bad_count/total_records*100:.2f}%")
```

---

## Patr√≥n 6: Procesamiento Complejo con Stage Variables

### DataStage: Transformer con Stage Variables

```basic
' DataStage Logic
' Stage Variables:
'   PrevCustomerID (initial: "") 
'   CustomerRecordCount (initial: 0)
'
' Derivations:
'   If CustomerID = PrevCustomerID Then
'     CustomerRecordCount = CustomerRecordCount + 1
'   Else
'     Begin
'       PrevCustomerID = CustomerID
'       CustomerRecordCount = 1
'     End
```

### Migraci√≥n PySpark usando Window Functions

```python
# COMMAND ----------
# MAGIC %md
# MAGIC ## Complex Transformation con L√≥gica de Secuencia

# COMMAND ----------

from pyspark.sql.window import Window

# DataStage Stage: Transformer_WithStageVars
# Stage Variables simuladas con Window Functions

# Definir ventana para particionar por CustomerID y ordenar
window_spec = Window.partitionBy("CustomerID").orderBy("TransactionDate")

df_with_sequence = df.withColumn(
    "CustomerRecordCount",
    F.row_number().over(window_spec)
).withColumn(
    "IsFirstTransaction",
    F.when(F.col("CustomerRecordCount") == 1, True).otherwise(False)
).withColumn(
    "RunningTotal",
    F.sum("Amount").over(window_spec.rowsBetween(Window.unboundedPreceding, Window.currentRow))
)

display(df_with_sequence)

# COMMAND ----------
# MAGIC %md
# MAGIC ## Detectar Cambios de Grupo (Change Detection)

# COMMAND ----------

# DataStage: Stage variable para detectar cambio de CustomerID
# PySpark: Usar lag function

window_all = Window.orderBy("TransactionDate")

df_with_changes = df.withColumn(
    "PrevCustomerID",
    F.lag ("CustomerID", 1).over(window_all)
).withColumn(
    "IsCustomerChange",
    F.when(
        (F.col("CustomerID") != F.col("PrevCustomerID")) | 
        F.col("PrevCustomerID").isNull(),
        True
    ).otherwise(False)
)

display(df_with_changes)
```

---

## Tips de Migraci√≥n

### 1. Preservar Logging y Metadata
```python
# Agregar metadata de procesamiento
df_with_metadata = df \
    .withColumn("process_timestamp", F.current_timestamp()) \
    .withColumn("source_file", F.input_file_name()) \
    .withColumn("job_name", F.lit("customer_etl")) \
    .withColumn("job_run_id", F.lit(dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()))
```

### 2. Validaciones de Calidad
```python
# Validar counts antes y despu√©s
assert df_output.count() == expected_count, f"Count mismatch: expected {expected_count}, got {df_output.count()}"

# Checksum validation
checksum_before = df_input.select(F.sum(F.hash(*df_input.columns))).collect()[0][0]
checksum_after = df_output.select(F.sum(F.hash(*df_output.columns))).collect()[0][0]
```

### 3. Performance Optimization
```python
# Repartition antes de writes costosos
df.repartition(200).write...

# Persist DataFrames que se usan m√∫ltiples veces
df_cached = df.cache()
df_cached.count()  # Trigger caching

# Cleanup despu√©s de uso
df_cached.unpersist()
```

### 4. Manejo de Par√°metros DataStage
```python
# DataStage Job Parameters ‚Üí Databricks Widgets
dbutils.widgets.text("DS_INPUT_PATH", "")
dbutils.widgets.text("DS_OUTPUT_PATH", "")
dbutils.widgets.text("DS_RUN_DATE", "")
dbutils.widgets.dropdown("DS_LOAD_TYPE", "FULL", ["FULL", "INCREMENTAL"])

# Usar en c√≥digo
input_path = dbutils.widgets.get("DS_INPUT_PATH")
load_type = dbutils.widgets.get("DS_LOAD_TYPE")
```


---

## knowledge/databricks-best-practices.md

# Mejores Pr√°cticas de Databricks para Migraciones desde DataStage

Este documento cubre las mejores pr√°cticas espec√≠ficas de Databricks al migrar desde DataStage.

## Delta Lake - El Fundamento

Delta Lake es crucial para reemplazar capacidades de DataStage datasets y tablas intermedias.

### Ventajas sobre archivos tradicionales
- **ACID Transactions**: Garantiza consistencia
- **Time Travel**: Auditor√≠a y rollback f√°cil
- **Schema Evolution**: Evoluci√≥n de esquema autom√°tica
- **Unified Batch & Streaming**: Simplifica arquitectura
- **Compaction & Optimization**: Mejor performance

### Operaciones B√°sicas

```python
# Leer Delta Table
df = spark.read.format("delta").load("/mnt/delta/customers")

# Escribir Delta Table (sobrescribir)
df.write.format("delta") \
    .mode("overwrite") \
    .option("overwriteSchema", "true") \
    .save("/mnt/delta/customers")

# Escribir Delta Table (append)
df.write.format("delta") \
    .mode("append") \
    .save("/mnt/delta/customers")

# Merge (UPSERT) - Reemplaza DataStage Update/Insert stages
from delta.tables import DeltaTable

delta_table = DeltaTable.forPath(spark, "/mnt/delta/customers")
delta_table.alias("target").merge(
    df_updates.alias("updates"),
    "target.customer_id = updates.customer_id"
).whenMatchedUpdateAll() \
 .whenNotMatchedInsertAll() \
 .execute()
```

### Time Travel y Auditor√≠a

```python
# DataStage no tiene equivalente nativo - Delta Lake lo hace f√°cil

# Leer versi√≥n hist√≥rica
df_yesterday = spark.read.format("delta") \
    .option("versionAsOf", 5) \
    .load("/mnt/delta/customers")

# O por timestamp
df_yesterday = spark.read.format("delta") \
    .option("timestampAsOf", "2024-01-01") \
    .load("/mnt/delta/customers")

# Ver historial
delta_table = DeltaTable.forPath(spark, "/mnt/delta/customers")
display(delta_table.history())

# Restaurar versi√≥n anterior
delta_table.restoreToVersion(5)
```

### Optimizaci√≥n Delta

```python
# Compactar archivos peque√±os (DataStage no lo necesita, pero cr√≠tico en Databricks)
spark.sql("OPTIMIZE delta.`/mnt/delta/customers`")

# Con Z-Ordering (mejor para queries espec√≠ficas)
spark.sql("OPTIMIZE delta.`/mnt/delta/customers` ZORDER BY (customer_id, region)")

# Vacuum - limpiar archivos antiguos (cuidado con time travel)
spark.sql("VACUUM delta.`/mnt/delta/customers` RETAIN 168 HOURS")  # 7 d√≠as
```

---

## Particionamiento - Cr√≠tico para Performance

En DataStage, el particionamiento es m√°s simple. En Spark/Databricks es fundamental.

### Tipos de Particionamiento

**1. Particionamiento F√≠sico (Storage)**
```python
# Particionar por columna al escribir (similar a DataStage partitioning)
df.write.format("delta") \
    .partitionBy("year", "month") \
    .save("/mnt/delta/transactions")

# Beneficio: Queries con filtros en estas columnas ser√°n mucho m√°s r√°pidas
# WHERE year = 2024 AND month = 1  <- Lee solo esa partici√≥n f√≠sica
```

**2. Particionamiento en Memoria (DataFrame)**
```python
# Repartition - full shuffle (costoso)
df_repartitioned = df.repartition(200)  # 200 particiones
df_repartitioned = df.repartition(200, "customer_id")  # Por columna

# Coalesce - reduce particiones sin shuffle (m√°s eficiente)
df_coalesced = df.coalesce(10)

# Cu√°ndo usar qu√©:
# - Repartition: Antes de operaciones pesadas (joins, aggregations)
# - Coalesce: Antes de escribir, para reducir n√∫mero de archivos
```

### Gu√≠a de Particionamiento

```python
# ‚ùå MAL: Demasiadas particiones peque√±as
df.repartition(10000).write...  # Miles de archivos peque√±os

# ‚úÖ BIEN: Balance entre paralelismo y overhead
num_cores = spark.sparkContext.defaultParallelism
df.repartition(num_cores * 2).write...

# Para writes, consolidar:
df.repartition(num_cores * 2) \
    .write \
    .coalesce(50) \  # 50 archivos finales
    .save(...)
```

### Migrar DataStage Partitioning

| DataStage Partition | Databricks Equivalent |
|---------------------|----------------------|
| Auto | `df` (sin repartition expl√≠cito) |
| Hash | `df.repartition("key_column")` |
| Modulus | `df.repartition(N, "key_column")` |
| Range | `df.repartitionByRange("sort_column")` |
| Round Robin | `df.repartition(N)` |
| Same | No hacer nada (mantener particionamiento) |
| Entire | `df.coalesce(1)` |

---

## Broadcast Joins - Optimizaci√≥n Clave

DataStage Lookup stages ‚Üí Databricks Broadcast Joins

### Cu√°ndo Usar Broadcast
- Tabla peque√±a (< 10GB, idealmente < 1GB)
- Join con tabla grande
- Evita shuffle costoso de la tabla grande

```python
from pyspark.sql.functions import broadcast

# ‚úÖ BIEN: Small reference table
df_result = df_large.join(
    broadcast(df_small_reference),
    on="key",
    how="left"
)

# ‚ùå MAL: Broadcast de tabla grande
df_result = df_large1.join(
    broadcast(df_large2),  # Falla o muy lento
    on="key"
)
```

### Auto Broadcast Configuration
```python
# Configurar threshold de auto-broadcast
spark.conf.set("spark.sql.autoBroadcastJoinThreshold", "100MB")

# Disable auto-broadcast si quieres control manual
spark.conf.set("spark.sql.autoBroadcastJoinThreshold", "-1")
```

---

## Caching y Persistence

DataStage mantiene datos en memoria impl√≠citamente. En Spark es expl√≠cito.

### Cu√°ndo Usar Cache

```python
# Usar cache cuando DataFrame se usa m√∫ltiples veces
df_important = df.filter(...).select(...)

# Sin cache - se recomputa cada vez
df_important.count()  # Compute
df_important.show()   # Compute again
df_important.write... # Compute again!

# Con cache - se computa una vez
df_important = df.filter(...).select(...).cache()
df_important.count()  # Compute and cache
df_important.show()   # Read from cache
df_important.write... # Read from cache

# IMPORTANTE: Unpersist cuando termines
df_important.unpersist()
```

### Niveles de Storage

```python
from pyspark import StorageLevel

# Memory only (default para .cache())
df.persist(StorageLevel.MEMORY_ONLY)

# Memory and disk (si no cabe en memoria, spill a disco)
df.persist(StorageLevel.MEMORY_AND_DISK)

# Disk only
df.persist(StorageLevel.DISK_ONLY)

# Con serializaci√≥n (menor memoria, m√°s CPU)
df.persist(StorageLevel.MEMORY_AND_DISK_SER)
```

---

## PySpark Optimizations

### 1. Evitar UDFs Python Cuando Sea Posible

```python
# ‚ùå LENTO: UDF Python
from pyspark.sql.functions import udf
from pyspark.sql.types import IntegerType

@udf(returnType=IntegerType())
def calculate_age_udf(birth_year):
    return 2024 - birth_year

df = df.withColumn("age", calculate_age_udf("birth_year"))

# ‚úÖ R√ÅPIDO: Native Spark functions
df = df.withColumn("age", F.lit(2024) - F.col("birth_year"))
```

### 2. Usar Built-in Functions

```python
from pyspark.sql import functions as F

# String operations
df.withColumn("name_upper", F.upper(F.col("name")))
df.withColumn("name_clean", F.trim(F.regexp_replace("name", "[^a-zA-Z]", "")))

# Date operations
df.withColumn("year", F.year("date_column"))
df.withColumn("days_diff", F.datediff(F.current_date(), "date_column"))

# Null handling
df.withColumn("clean_value", F.coalesce("column1", "column2", F.lit("default")))

# Conditional logic
df.withColumn("category",
    F.when(F.col("amount") > 1000, "HIGH")
     .when(F.col("amount") > 100, "MEDIUM")
     .otherwise("LOW")
)
```

### 3. Preferir DataFrame API sobre RDD

```python
# ‚ùå EVITAR: RDD operations
rdd_result = df.rdd.map(lambda row: complex_logic(row))

# ‚úÖ PREFERIR: DataFrame API con expresiones SQL
df_result = df.select(
    "*",
    F.expr("complex SQL expression").alias("result")
)
```

---

## Databricks Notebooks Best Practices

### Estructura Recomendada

```python
# COMMAND ----------
# MAGIC %md
# MAGIC # Job Title
# MAGIC Description, migration date, source DS job

# COMMAND ----------
# MAGIC %md
# MAGIC ## üìù Parameters

# COMMAND ----------
# Widgets para par√°metros (reemplaza DataStage job parameters)
dbutils.widgets.text("input_path", "", "Input Path")
dbutils.widgets.text("output_path", "", "Output Path")
dbutils.widgets.dropdown("load_type", "FULL", ["FULL", "INCREMENTAL"], "Load Type")

# COMMAND ----------
# MAGIC %md
# MAGIC ## üì¶ Imports and Configuration

# COMMAND ----------
from pyspark.sql import functions as F
from pyspark.sql.types import *
from delta.tables import DeltaTable
import logging

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# COMMAND ----------
# MAGIC %md
# MAGIC ## üîß Configuration

# COMMAND ----------
# Spark configurations
spark.conf.set("spark.sql.adaptive.enabled", "true")
spark.conf.set("spark.sql.adaptive.coalescePartitions.enabled", "true")

# COMMAND ----------
# MAGIC %md
# MAGIC ## üìä Data Processing
# MAGIC ### Stage 1: Read Input

# COMMAND ----------
# ... processing code ...

# COMMAND ----------
# MAGIC %md
# MAGIC ### Stage 2: Transform

# COMMAND ----------
# ... transformation code ...

# COMMAND ----------
# MAGIC %md
# MAGIC ## ‚úÖ Validation and Logging

# COMMAND ----------
# Final validation and metrics
```

### Markdown para Documentaci√≥n

```python
# COMMAND ----------
# MAGIC %md
# MAGIC ## DataStage Stage Mapping
# MAGIC 
# MAGIC | DataStage Stage | Type | Databricks Implementation |
# MAGIC |----------------|------|---------------------------|
# MAGIC | Input_Customers | Sequential File | spark.read.csv() |
# MAGIC | Transform_Clean | Transformer | withColumn() operations |
# MAGIC | Aggregate_Sales | Aggregator | groupBy().agg() |
# MAGIC | Output_Results | Dataset | Delta Lake write |
```

### Widgets para Par√°metros

```python
# Tipos de widgets
dbutils.widgets.text("param_string", "default", "String Parameter")
dbutils.widgets.dropdown("param_choice", "A", ["A", "B", "C"], "Choice Parameter")
dbutils.widgets.combobox("param_combo", "default", ["opt1", "opt2"], "Combo Parameter")
dbutils.widgets.multiselect("param_multi", "A", ["A", "B", "C"], "Multi Select")

# Obtener valores
value = dbutils.widgets.get("param_string")

# Remover widgets (√∫til al finalizar)
dbutils.widgets.removeAll()
```

---

## Monitoring y Logging

### Logging Estructurado

```python
import logging
from datetime import datetime

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Log events
logger.info(f"Starting job: {job_name}")
logger.info(f"Processing date: {run_date}")
logger.info(f"Records read: {df.count()}")
logger.warning(f"High reject rate: {reject_rate}%")
logger.error(f"Validation failed: {error_message}")

# Exception handling con logging
try:
    result = process_data()
    logger.info("Processing completed successfully")
except Exception as e:
    logger.error(f"Error in processing: {str(e)}", exc_info=True)
    raise
```

### M√©tricas y Validaci√≥n

```python
# Capturar m√©tricas
metrics = {
    "job_name": "customer_etl",
    "run_timestamp": datetime.now(),
    "input_records": df_input.count(),
    "output_records": df_output.count(),
    "rejected_records": df_reject.count(),
    "processing_time_seconds": processing_time
}

# Escribir m√©tricas a tabla
metrics_df = spark.createDataFrame([metrics])
metrics_df.write.format("delta") \
    .mode("append") \
    .save("/mnt/metrics/job_metrics")

# Validaciones
assert df_output.count() > 0, "No records in output"
assert df_output.count() == expected_count, f"Count mismatch"

# Data quality checks
null_check = df_output.filter(F.col("key_column").isNull()).count()
assert null_check == 0, f"Found {null_check} null keys"
```

---

## Performance Tuning Tips

### 1. Adaptive Query Execution (AQE)

```python
# Habilitar AQE (recomendado para todas las migraciones)
spark.conf.set("spark.sql.adaptive.enabled", "true")
spark.conf.set("spark.sql.adaptive.coalescePartitions.enabled", "true")
spark.conf.set("spark.sql.adaptive.skewJoin.enabled", "true")
```

### 2. Configuraci√≥n de Shuffle

```python
# Ajustar particiones de shuffle para tu dataset
spark.conf.set("spark.sql.shuffle.partitions", "200")  # Default

# Guideline: 
# - Small data (< 1GB): 50-100
# - Medium data (1-10GB): 200-400
# - Large data (> 10GB): 400-2000
```

### 3. Evitar Data Skew

```python
# Problema: Una partici√≥n mucho m√°s grande que otras
# Soluci√≥n 1: A√±adir salt a la key
df_salted = df.withColumn("salt", (F.rand() * 10).cast("int"))
df_salted = df_salted.withColumn("salted_key", F.concat("key", F.lit("_"), "salt"))

# Join con salted key
df_result = df_salted.join(df_other_salted, on="salted_key")

# Soluci√≥n 2: Broadcast si una tabla es peque√±a
df_result = df_large.join(broadcast(df_small), on="key")

# Soluci√≥n 3: AQE maneja skew autom√°ticamente
spark.conf.set("spark.sql.adaptive.skewJoin.enabled", "true")
```

### 4. Minimizar Shuffles

```python
# ‚ùå MALO: M√∫ltiples shuffles
df.repartition(100) \
    .groupBy("key") \  # Shuffle 1
    .agg(...) \
    .join(df2, "key") \  # Shuffle 2
    .orderBy("value")  # Shuffle 3

# ‚úÖ MEJOR: Planificar particionamiento
df.repartition(100, "key") \  # Particionar por key una vez
    .groupBy("key") \  # No shuffle (ya particionado por key)
    .agg(...) \
    .join(df2.repartition("key"), "key") \  # Shuffle reducido
    .orderBy("value")
```

---

## Security y Governance

### Databricks Secrets

```python
# NO hacer esto - credentials en c√≥digo
username = "admin"
password = "password123"  # ‚ùå NUNCA

# Usar Databricks Secrets
username = dbutils.secrets.get(scope="jdbc-scope", key="username")
password = dbutils.secrets.get(scope="jdbc-scope", key="password")

# Usar en JDBC connection
df = spark.read.format("jdbc") \
    .option("url", jdbc_url) \
    .option("user", username) \
    .option("password", password) \
    .load()
```

### Unity Catalog

```python
# Usar tablas catalogadas en lugar de paths
# ‚ùå Antiguo: Paths directos
df = spark.read.format("delta").load("/mnt/data/customers")

# ‚úÖ Moderno: Unity Catalog
df = spark.read.table("catalog.schema.customers")

# Write to catalog
df.write.mode("overwrite").saveAsTable("catalog.schema.customers")
```

---

## Testing y Validaci√≥n

### Unit Tests para Transformaciones

```python
# Crear test data
test_data = [
    (1, "John", "2000-01-01", 100),
    (2, "Jane", None, 200),
    (3, "Bob", "1990-15-99", -50),  # Invalid date
]
test_df = spark.createDataFrame(test_data, ["id", "name", "birth_date", "amount"])

# Aplicar transformaci√≥n
result_df = apply_transformation(test_df)

# Validar resultados
assert result_df.count() == expected_count
assert result_df.filter(F.col("amount") < 0).count() == 0
```

### Comparaci√≥n con DataStage Output

```python
# Para validar migraci√≥n correcta
df_datastage_output = spark.read.csv("/mnt/datastage_output/")
df_databricks_output = spark.read.format("delta").load("/mnt/databricks_output/")

# Comparar schemas
print("DataStage schema:", df_datastage_output.schema)
print("Databricks schema:", df_databricks_output.schema)

# Comparar counts
ds_count = df_datastage_output.count()
db_count = df_databricks_output.count()
print(f"Count difference: {db_count - ds_count}")

# Comparar registros espec√≠ficos
diff = df_databricks_output.subtract(df_datastage_output)
print(f"Records only in Databricks: {diff.count()}")
```

---

## Migration Checklist

Cuando migres un DataStage job, verifica:

‚úÖ **Funcionalidad**
- [ ] Todas las transformaciones migradas
- [ ] Todas las validaciones implementadas
- [ ] Error handling equivalente
- [ ] Logging apropiado

‚úÖ **Performance**
- [ ] Particionamiento optimizado
- [ ] Broadcast joins donde apropiado
- [ ] Caching para DataFrames reutilizados
- [ ] AQE habilitado

‚úÖ **Data Quality**
- [ ] Validaciones de schema
- [ ] Null handling correcto
- [ ] Validaciones de counts
- [ ] Checksums si necesario

‚úÖ **Operacional**
- [ ] Par√°metros como widgets
- [ ] Secrets para credentials
- [ ] Logging estructurado
- [ ] M√©tricas capturadas
- [ ] Documentaci√≥n clara

‚úÖ **Testing**
- [ ] Tests con data de muestra
- [ ] Comparaci√≥n con output DataStage
- [ ] Validaci√≥n end-to-end
- [ ] Performance testing


---

## knowledge/quick-migration-guide.md

# Gu√≠a R√°pida de Migraci√≥n DataStage ‚Üí Databricks

Esta gu√≠a proporciona un proceso paso a paso para migrar jobs de DataStage a Databricks usando el agente de Copilot.

## üéØ Proceso de Migraci√≥n (30 minutos - 2 horas por job)

### Fase 1: Preparaci√≥n (5-10 min)

#### 1.1 Exportar Job DataStage
```bash
# Opci√≥n A: Desde DataStage Designer
File ‚Üí Export ‚Üí DataStage Components ‚Üí Select Job ‚Üí Export as DSX

# Opci√≥n B: Comando (DataStage Server)
dsadmin -DSExport -domain <domain> -user <user> -password <pass> \
  -project <project> -jobname <jobname> -file <output.dsx>
```

#### 1.2 Reunir Informaci√≥n del Job
Documentar:
- [ ] Prop√≥sito del job
- [ ] Frecuencia de ejecuci√≥n (diaria, horaria, etc.)
- [ ] Volumen de datos t√≠pico
- [ ] Dependencias (jobs upstream/downstream)
- [ ] Par√°metros y sus valores t√≠picos
- [ ] SLAs y requisitos de performance

#### 1.3 Preparar Entorno Databricks
- [ ] Crear o seleccionar workspace Databricks
- [ ] Configurar cluster (tama√±o apropiado)
- [ ] Configurar Delta Lake paths (si aplica)
- [ ] Configurar Secrets para credentials
- [ ] Crear schemas en Unity Catalog (recomendado)

---

### Fase 2: Migraci√≥n con Agente (10-30 min)

#### 2.1 Usar GitHub Copilot Chat

Copiar el contenido del archivo DSX y pegar en el chat:

```
@workspace Tengo este job DataStage que necesito migrar a Databricks:

[pegar contenido del DSX aqu√≠]

Por favor:
1. Analiza todos los stages y su flujo
2. Genera un notebook Databricks completo
3. Incluye optimizaciones para Spark
4. Agrega logging y error handling
5. Documenta las decisiones de dise√±o
```

#### 2.2 Alternativa: Proporcionar Descripci√≥n

Si no tienes el DSX, describe el job:

```
@workspace Necesito migrar un job DataStage con esta estructura:

- Input: Lee CSV con √≥rdenes desde S3
- Join: Une con tabla de clientes (inner join)
- Lookup: Busca productos en tabla de referencia peque√±a
- Aggregator: Suma ventas por cliente y mes
- Output: Escribe a Delta Lake

Incluye:
- Par√°metros para rutas de input/output
- Validaciones de calidad de datos
- Manejo de errores
- Monitoreo
```

#### 2.3 Revisi√≥n del C√≥digo Generado

El agente te dar√°:
1. **An√°lisis del Job**: Resumen de stages y complejidad
2. **Notebook Databricks**: C√≥digo PySpark completo
3. **Notas de Migraci√≥n**: Decisiones importantes
4. **Recomendaciones**: Optimizaciones y tests

Revisar:
- [ ] Todas las transformaciones est√°n migradas
- [ ] Joins tienen estrategia correcta (broadcast vs shuffle)
- [ ] Particionamiento es apropiado
- [ ] Error handling est√° implementado
- [ ] Par√°metros son configurable (widgets)

---

### Fase 3: Implementaci√≥n (15-30 min)

#### 3.1 Crear Notebook en Databricks

1. Ir a Databricks Workspace
2. Create ‚Üí Notebook
3. Nombrar: `[JobName]_Migrated`
4. Copiar c√≥digo generado por el agente
5. Guardar

#### 3.2 Configurar Par√°metros

```python
# Crear widgets para todos los par√°metros DataStage
dbutils.widgets.text("input_path", "", "Input Path")
dbutils.widgets.text("output_path", "", "Output Path")
dbutils.widgets.dropdown("load_type", "FULL", ["FULL", "INCREMENTAL"])

# Si usas secrets
input_path = dbutils.secrets.get("scope", "input_path_key")
# O desde widgets
input_path = dbutils.widgets.get("input_path")
```

#### 3.3 Ajustar Configuraci√≥n de Spark

```python
# Adaptar seg√∫n tama√±o de datos
spark.conf.set("spark.sql.shuffle.partitions", "200")  # Ajustar seg√∫n volumen
spark.conf.set("spark.sql.adaptive.enabled", "true")
spark.conf.set("spark.sql.adaptive.coalescePartitions.enabled", "true")
```

#### 3.4 Crear Tablas Delta (si aplica)

```python
# Si usas Unity Catalog
spark.sql("""
  CREATE TABLE IF NOT EXISTS catalog.schema.table_name (
    column1 STRING,
    column2 INT,
    ...
  )
  USING DELTA
  PARTITIONED BY (partition_column)
""")
```

---

### Fase 4: Testing (15-30 min)

#### 4.1 Test con Dataset Peque√±o

```python
# Usar subset de datos para testing r√°pido
test_input = "/path/to/test_data_small.csv"
test_output = "/tmp/test_output"

# Ejecutar notebook con test data
```

#### 4.2 Validar Output

```python
# Comparar con output DataStage si disponible
df_databricks = spark.read.format("delta").load(test_output)
df_datastage = spark.read.csv("/path/to/datastage_output")

# Comparar counts
print(f"Databricks count: {df_databricks.count()}")
print(f"DataStage count: {df_datastage.count()}")

# Comparar sample
display(df_databricks.limit(10))
display(df_datastage.limit(10))

# Comparar checksums (columnas num√©ricas)
databricks_sum = df_databricks.agg({"numeric_col": "sum"}).collect()[0][0]
datastage_sum = df_datastage.agg({"numeric_col": "sum"}).collect()[0][0]
print(f"Sum difference: {databricks_sum - datastage_sum}")
```

#### 4.3 Tests de Calidad de Datos

```python
# Verificar nulls en columnas clave
null_checks = df_databricks.select(
    *[F.sum(F.col(c).isNull().cast("int")).alias(f"{c}_nulls") 
      for c in df_databricks.columns]
).collect()[0].asDict()

for col, null_count in null_checks.items():
    if null_count > 0:
        print(f"‚ö†Ô∏è {col}: {null_count} null values")

# Verificar duplicados
dup_count = df_databricks.groupBy("key_column").count() \
    .filter(F.col("count") > 1).count()
if dup_count > 0:
    print(f"‚ö†Ô∏è Found {dup_count} duplicate keys")
```

---

### Fase 5: Optimizaci√≥n (Opcional, 10-20 min)

#### 5.1 Analizar Performance

```python
# Ejecutar con datos completos
# Revisar Spark UI para identificar bottlenecks

# Queries lentas
spark.sql("EXPLAIN COST SELECT * FROM ...").show()

# Identificar shuffles costosos
# Ver Spark UI ‚Üí Stages ‚Üí identificar stages con mucho shuffle
```

#### 5.2 Aplicar Optimizaciones

```python
# Si hay join lento y una tabla es peque√±a
df_result = df_large.join(broadcast(df_small), on="key")

# Si hay aggregation lenta
# Reparticionar antes del groupBy
df.repartition("partition_key").groupBy("partition_key").agg(...)

# Compactar archivos Delta
spark.sql("OPTIMIZE delta.`/path/to/table`")
spark.sql("OPTIMIZE delta.`/path/to/table` ZORDER BY (commonly_filtered_column)")
```

#### 5.3 Tuning de Cluster

- Aumentar workers si proceso es muy lento
- Usar instance types con m√°s memoria si hay OOM errors
- Habilitar autoscaling para cargas variables

---

### Fase 6: Deployment (10-15 min)

#### 6.1 Crear Job en Databricks

1. Databricks UI ‚Üí Workflows ‚Üí Create Job
2. Configurar:
   - **Task name**: Descriptivo
   - **Type**: Notebook
   - **Notebook path**: Seleccionar notebook migrado
   - **Cluster**: Seleccionar o crear
   - **Parameters**: Agregar key-value pairs para widgets

3. Schedule (si aplica):
   - **Trigger type**: Scheduled
   - **Cron expression**: Equivalente al schedule DataStage

#### 6.2 Configurar Alertas

```python
# En el notebook, agregar al final
success_message = f"""
‚úÖ Job completado exitosamente
- Records processed: {record_count}
- Execution time: {execution_time} seconds
- Output path: {output_path}
"""

# Enviar notificaci√≥n (ejemplo con Slack webhook)
import requests
requests.post(slack_webhook_url, json={"text": success_message})

# O usar Databricks Job notifications
# (configurar en Job UI ‚Üí Alerts)
```

#### 6.3 Monitoreo

- Configurar alertas en Job failures
- Revisar m√©tricas regularmente
- Guardar logs en tabla para an√°lisis:

```python
# Al final del notebook
job_metrics = spark.createDataFrame([{
    "job_name": "customer_etl",
    "execution_date": datetime.now(),
    "status": "SUCCESS",
    "records_processed": record_count,
    "execution_time_sec": execution_time,
    "errors_count": error_count
}])

job_metrics.write.format("delta").mode("append") \
    .saveAsTable("catalog.monitoring.job_metrics")
```

---

## üéì Tips y Mejores Pr√°cticas

### Durante la Migraci√≥n

1. **Empezar Simple**: Migrar jobs peque√±os primero para familiarizarte
2. **Documentar Todo**: Decisiones, diferencias con DataStage, problemas encontrados
3. **Comparar Outputs**: Validar con data de prueba contra output DataStage original
4. **Optimizar Gradualmente**: Funcionalidad primero, optimizaci√≥n despu√©s
5. **Reusar Patrones**: Crear templates para patrones comunes (SCD, validation, etc.)

### Patrones Comunes a Conocer

| DataStage Pattern | Databricks Pattern |
|------------------|-------------------|
| Sequential File ‚Üí Dataset | CSV/Parquet ‚Üí Delta Lake |
| Lookup (small table) | Broadcast Join |
| Join (large tables) | Repartition Join |
| Aggregator | groupBy().agg() |
| Sort | orderBy() |
| Remove Duplicates | dropDuplicates() |
| Change Capture + SCD2 | Delta Lake Merge |
| Reject Links | filter() + separate DataFrames |
| Stage Variables | Columnas temporales o Window functions |

### Errores Comunes a Evitar

‚ùå **No optimizar joins**
```python
# Lento si ambas tablas son grandes
df1.join(df2, on="key")  
```
‚úÖ **Usar broadcast para tablas peque√±as**
```python
df_large.join(broadcast(df_small), on="key")
```

‚ùå **No particionar antes de writes**
```python
df.write.save(path)  # Miles de archivos peque√±os
```
‚úÖ **Coalesce para consolidar**
```python
df.coalesce(50).write.save(path)  # 50 archivos
```

‚ùå **Usar Python UDFs innecesariamente**
```python
@udf
def calculate(x):
    return x * 2
df.withColumn("result", calculate("value"))
```
‚úÖ **Usar funciones nativas de Spark**
```python
df.withColumn("result", F.col("value") * 2)
```

---

## üìä Checklist de Migraci√≥n Completa

### Funcionalidad
- [ ] Todas las transformaciones migradas
- [ ] Joins/Lookups correctamente implementados
- [ ] Aggregations con todas las funciones requeridas
- [ ] Filtros y constraints aplicados
- [ ] Error handling equivalente a DataStage

### Performance
- [ ] Particionamiento optimizado
- [ ] Broadcast joins donde apropiado
- [ ] Shuffle minimizado
- [ ] AQE (Adaptive Query Execution) habilitado
- [ ] Delta Lake usado para tablas intermedias

### Operaciones
- [ ] Par√°metros como widgets configurables
- [ ] Secrets para credentials
- [ ] Logging implementado
- [ ] M√©tricas capturadas
- [ ] Alertas configuradas

### Testing
- [ ] Comparado con output DataStage
- [ ] Validaciones de calidad ejecutadas
- [ ] Tests con vol√∫menes representativos
- [ ] Performance aceptable
- [ ] Documentaci√≥n actualizada

### Deployment
- [ ] Job creado en Databricks
- [ ] Schedule configurado
- [ ] Dependencias documentadas
- [ ] Runbook creado
- [ ] Team entrenado

---

## üÜò Troubleshooting

### Problema: Job falla con Out of Memory

**Soluci√≥n**:
```python
# 1. Reducir shuffle partitions si dataset es peque√±o
spark.conf.set("spark.sql.shuffle.partitions", "50")

# 2. Aumentar memoria del driver/executors
# (configurar en cluster settings)

# 3. Procesar en batches
for batch in batches:
    process(batch)
    # Limpiar cache entre batches
    spark.catalog.clearCache()
```

### Problema: Job muy lento comparado con DataStage

**Investigar**:
1. Revisar Spark UI para identificar stage lento
2. Verificar si hay data skew (una partici√≥n mucho mayor que otras)
3. Verificar si broadcast join fall√≥ (tabla muy grande)
4. Revisar n√∫mero de particiones

**Soluciones**:
```python
# Repartition por key que distribuya uniformemente
df.repartition(200, "key_column")

# Forzar broadcast si tabla es <10GB
df.join(broadcast(df_small), on="key")

# Habilitar AQE para optimizaciones autom√°ticas
spark.conf.set("spark.sql.adaptive.enabled", "true")
spark.conf.set("spark.sql.adaptive.skewJoin.enabled", "true")
```

### Problema: Resultados diferentes a DataStage

**Verificar**:
1. Orden de operaciones (sort puede cambiar resultados)
2. Manejo de nulls (comportamiento diferente)
3. Conversiones de tipo (casting)
4. Truncamiento de fechas/timestamps
5. Funciones de string (√≠ndices base-0 vs base-1)

**Debug**:
```python
# Comparar stage por stage
df_after_stage1 = ...
display(df_after_stage1.limit(10))

# Comparar checksums
df.select(F.sum(F.hash(*df.columns))).show()
```

---

## üìö Recursos Adicionales

- **Knowledge Base**: `knowledge/` folder con patrones detallados
- **Test Artifacts**: `test-artifacts/` con jobs de ejemplo
- **Databricks Docs**: [docs.databricks.com](https://docs.databricks.com)
- **Delta Lake Docs**: [delta.io](https://delta.io)
- **PySpark API**: [spark.apache.org/docs/latest/api/python](https://spark.apache.org/docs/latest/api/python)

---

## üí° Pr√≥ximos Pasos Despu√©s de la Migraci√≥n

1. **Monitoreo Continuo** (Semanas 1-4)
   - Comparar outputs con DataStage daily
   - Monitorear performance y costos
   - Ajustar configuraciones seg√∫n necesidad

2. **Optimizaci√≥n** (Mes 1-2)
   - Identificar jobs que consumen m√°s recursos
   - Aplicar optimizaciones avanzadas
   - Implementar caching strategies

3. **Decommissioning DataStage** (Mes 2-6)
   - Migrar jobs restantes
   - Ejecutar ambos sistemas en paralelo
   - Validar completamente antes de shutdown
   - Documentar lessons learned


---

